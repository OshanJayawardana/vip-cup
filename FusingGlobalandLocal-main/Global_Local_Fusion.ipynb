{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OshanJayawardana/vip-cup/blob/main/FusingGlobalandLocal-main/Global_Local_Fusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8-L7xnphv0F"
      },
      "outputs": [],
      "source": [
        "#https://drive.google.com/file/d/1tQ4tfBWgJiEQaneXANlsXY6X3B8sNN1h/view\n",
        "#!gdown https://drive.google.com/uc?id=1tQ4tfBWgJiEQaneXANlsXY6X3B8sNN1h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1DrHzCLiFho"
      },
      "outputs": [],
      "source": [
        "#!unzip \"/content/FusingGlobalandLocal/model (1).zip\" -d \"/content/model/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "0K75hrPRQa54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d90j3XtiIks",
        "outputId": "2b5ca2cd-20a8-4b99-b28f-b9d3ecfbbbd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAykYRsiDzNf"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/gdrive/MyDrive/Datasets/pro_dataset.zip\" -d \"/content/pro_data/\" &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfrq_O1684KR",
        "outputId": "931f0291-bf09-4961-b73b-bf8da8e6f78b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123322 original real images\n",
            "115999 original fake images\n",
            "\n"
          ]
        }
      ],
      "source": [
        "REAL = \"/content/pro_data/data_pro/real/\"\n",
        "FAKE = \"/content/pro_data/data_pro/synt/\"\n",
        "\n",
        "print(f\"{len(os.listdir(REAL))} original real images\")\n",
        "print(f\"{len(os.listdir(FAKE))} original fake images\\n\")\n",
        "\n",
        "# 123322 original real images\n",
        "# 115999 original fake images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWwkfgpMbigW"
      },
      "outputs": [],
      "source": [
        "root_dir = '/content/dataset'\n",
        "os.makedirs(root_dir)\n",
        "\n",
        "def create_train_val_dirs(root_path):\n",
        "  path_train_real = os.path.join(root_path, \"train/0_real\")\n",
        "  path_val_real = os.path.join(root_path, \"val/0_real\")\n",
        "  path_train_fake = os.path.join(root_path, \"train/1_fake\")\n",
        "  path_val_fake = os.path.join(root_path, \"val/1_fake\")\n",
        "\n",
        "  os.makedirs(path_train_real)\n",
        "  os.makedirs(path_val_real)\n",
        "  os.makedirs(path_train_fake)\n",
        "  os.makedirs(path_val_fake)\n",
        "  \n",
        "create_train_val_dirs(root_path=root_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hQFIMPjdBps"
      },
      "outputs": [],
      "source": [
        "def split_data(SOURCE, TRAINING, VALIDATION, SPLIT_SIZE):\n",
        "\n",
        "  files = []\n",
        "  content_lst = os.listdir(SOURCE)\n",
        "\n",
        "  for file in content_lst:\n",
        "    file_path = os.path.join(SOURCE, file)\n",
        "    if os.path.getsize(file_path) == 0:\n",
        "      print(\"filename is zero length, so ignoring.\")\n",
        "    else:\n",
        "      files.append(file)\n",
        "\n",
        "  training_len = int(len(files) * SPLIT_SIZE)\n",
        "  val_len = training_len + int(len(files) * ((1 - SPLIT_SIZE)/2))\n",
        "  shuffled_lst = random.sample(files, len(files))\n",
        "  print(shuffled_lst[:10])\n",
        "  print(shuffled_lst[50:60])\n",
        "\n",
        "  train_set = shuffled_lst[ : training_len]\n",
        "  val_set = shuffled_lst[training_len : val_len]\n",
        "\n",
        "  for train_file in train_set: \n",
        "    train_path = os.path.join(SOURCE, train_file)\n",
        "    dest_path = os.path.join(TRAINING, train_file)\n",
        "    shutil.copyfile(train_path, dest_path)\n",
        "    os.remove(train_path)\n",
        "\n",
        "  for val_file in val_set: \n",
        "    val_path = os.path.join(SOURCE, val_file)\n",
        "    dest_path = os.path.join(VALIDATION, val_file)\n",
        "    shutil.copyfile(val_path, dest_path)\n",
        "    os.remove(val_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ria4yve3dY_v",
        "outputId": "ce406e22-002b-4584-b34b-2835cef3e34c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['img046282.jpg', 'img001646.jpg', 'img005932.jpg', 'img011243.jpg', 'img076624.jpg', 'img021984.jpg', 'img039940.jpg', 'img057234.jpg', 'img053862.jpg', 'img101162.jpg']\n",
            "['img056812.jpg', 'img100127.jpg', 'img050284.jpg', 'img011366.jpg', 'img036421.jpg', 'img021985.jpg', 'img033047.jpg', 'img078423.jpg', 'img087964.jpg', 'img047714.jpg']\n",
            "['img100272.jpg', 'img110758.jpg', 'img027952.jpg', 'img113151.jpg', 'img055311.jpg', 'img044279.jpg', 'img025792.jpg', 'img032109.jpg', 'img083317.jpg', 'img101079.jpg']\n",
            "['img073483.jpg', 'img017832.jpg', 'img115953.jpg', 'img102243.jpg', 'img103132.jpg', 'img070791.jpg', 'img022416.jpg', 'img039184.jpg', 'img013514.jpg', 'img075592.jpg']\n",
            "12333 remaining real images in original location \n",
            "11601 remaining fake images in original location\n",
            "\n",
            "98657 real images for training\n",
            "92799 fake images for training\n",
            "12332 real images for validation\n",
            "11599 fake images for validation\n"
          ]
        }
      ],
      "source": [
        "REAL_SOURCE_DIR = \"/content/pro_data/data_pro/real/\"\n",
        "FAKE_SOURCE_DIR = \"/content/pro_data/data_pro/synt/\"\n",
        "\n",
        "TRAINING_DIR = \"/content/dataset/train/\"\n",
        "VAL_DIR = \"/content/dataset/val/\"\n",
        "\n",
        "TRAINING_REAL_DIR = os.path.join(TRAINING_DIR, \"0_real/\")\n",
        "VAL_REAL_DIR = os.path.join(VAL_DIR, \"0_real/\")\n",
        "\n",
        "TRAINING_FAKE_DIR = os.path.join(TRAINING_DIR, \"1_fake/\")\n",
        "VAL_FAKE_DIR = os.path.join(VAL_DIR, \"1_fake/\")\n",
        "\n",
        "split_size = .8\n",
        "\n",
        "split_data(REAL_SOURCE_DIR, TRAINING_REAL_DIR, VAL_REAL_DIR, split_size)\n",
        "split_data(FAKE_SOURCE_DIR, TRAINING_FAKE_DIR, VAL_FAKE_DIR, split_size)\n",
        "\n",
        "print(f\"{len(os.listdir(REAL_SOURCE_DIR))} remaining real images in original location \")\n",
        "print(f\"{len(os.listdir(FAKE_SOURCE_DIR))} remaining fake images in original location\\n\")\n",
        "\n",
        "print(f\"{len(os.listdir(TRAINING_REAL_DIR))} real images for training\")\n",
        "print(f\"{len(os.listdir(TRAINING_FAKE_DIR))} fake images for training\")\n",
        "print(f\"{len(os.listdir(VAL_REAL_DIR))} real images for validation\")\n",
        "print(f\"{len(os.listdir(VAL_FAKE_DIR))} fake images for validation\")\n",
        "\n",
        "# ['img046282.jpg', 'img001646.jpg', 'img005932.jpg', 'img011243.jpg', 'img076624.jpg', 'img021984.jpg', 'img039940.jpg', 'img057234.jpg', 'img053862.jpg', 'img101162.jpg']\n",
        "# ['img056812.jpg', 'img100127.jpg', 'img050284.jpg', 'img011366.jpg', 'img036421.jpg', 'img021985.jpg', 'img033047.jpg', 'img078423.jpg', 'img087964.jpg', 'img047714.jpg']\n",
        "# ['img100272.jpg', 'img110758.jpg', 'img027952.jpg', 'img113151.jpg', 'img055311.jpg', 'img044279.jpg', 'img025792.jpg', 'img032109.jpg', 'img083317.jpg', 'img101079.jpg']\n",
        "# ['img073483.jpg', 'img017832.jpg', 'img115953.jpg', 'img102243.jpg', 'img103132.jpg', 'img070791.jpg', 'img022416.jpg', 'img039184.jpg', 'img013514.jpg', 'img075592.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdgmwi4h9aYP"
      },
      "outputs": [],
      "source": [
        "os.rename(\"/content/pro_data\",\"/content/test\")\n",
        "os.rename(\"/content/test/data_pro\",\"/content/test/general\")\n",
        "os.rename(\"/content/test/arch1/real\",\"/content/test/arch1/0_real\")\n",
        "os.rename(\"/content/test/arch1/synt\",\"/content/test/arch1/1_fake\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7583oTkfiVjt",
        "outputId": "b381e313-4e8f-440a-cec8-a6cef60246c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/FusingGlobalandLocal\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/gdrive/MyDrive/FusingGlobalandLocal\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRwuayB-j2zI",
        "outputId": "882bdb5f-3b94-4116-e81b-9e66a17e604f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahcQrxk0ih1m"
      },
      "outputs": [],
      "source": [
        "#!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SChY-jM2inhH",
        "outputId": "da9b29f0-c75e-481a-a721-e75155d89f00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qoH8YymoTqO",
        "outputId": "6ce2d7a9-f123-42cc-a616-75bf4a79dd2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyEjRlki7Ab9",
        "outputId": "6f772334-3ab0-45cb-e9e2-27b7ff76df57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (2.9.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0omFVDa8S0_",
        "outputId": "fd93f527-5f1c-403e-dc9e-d35222949a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akfk3zBT83_7",
        "outputId": "8416bb0f-2c9d-420a-a3a8-1eb7e679e577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python-util\n",
            "  Downloading python_util-1.2.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: python-util\n",
            "Successfully installed python-util-1.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install python-util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCc6_N_j7D7F"
      },
      "outputs": [],
      "source": [
        "import tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOTetHe6JXlR"
      },
      "outputs": [],
      "source": [
        "#!pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5oCSQj5fZXN"
      },
      "outputs": [],
      "source": [
        "# in paper\n",
        "# batch size - 64\n",
        "# lr - 0.0001\n",
        "# adam\n",
        "# loss - BcE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-iUn6aw8S9u",
        "outputId": "39dd5b03-bff2-4f51-ca6a-fddd5638fa1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- Options ---------------\n",
            "                     arch: res50                         \n",
            "               batch_size: 16                            \t[default: 64]\n",
            "                    beta1: 0.9                           \n",
            "                blur_prob: 0.1                           \t[default: 0]\n",
            "                 blur_sig: 0.0,3.0                       \t[default: 0.5]\n",
            "          checkpoints_dir: ./checkpoints                 \n",
            "                class_bal: False                         \n",
            "           continue_train: False                         \n",
            "                 cropSize: 200                           \n",
            "                 data_aug: False                         \n",
            "                 dataroot: /content/dataset              \t[default: ./dataset/]\n",
            "          earlystop_epoch: 10                            \n",
            "                    epoch: latest                        \n",
            "              epoch_count: 1                             \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                  isTrain: True                          \t[default: None]\n",
            "               jpg_method: cv2,pil                       \t[default: cv2]\n",
            "                 jpg_prob: 0.1                           \t[default: 0]\n",
            "                 jpg_qual: 30,100                        \t[default: 75]\n",
            "               last_epoch: -1                            \n",
            "                 loadSize: 256                           \n",
            "                loss_freq: 400                           \n",
            "                       lr: 0.0001                        \n",
            "                     mode: binary                        \n",
            "                     name: test_1                        \t[default: experiment_name]\n",
            "                new_optim: False                         \n",
            "                    niter: 5                             \t[default: 10000]\n",
            "                  no_flip: False                         \n",
            "              num_threads: 4                             \n",
            "                    optim: adam                          \n",
            "           resize_or_crop: scale_and_crop                \n",
            "                rz_interp: bilinear                      \n",
            "          save_epoch_freq: 20                            \n",
            "         save_latest_freq: 2000                          \n",
            "           serial_batches: False                         \n",
            "                   suffix:                               \n",
            "              train_split: train                         \n",
            "                val_split: val                           \n",
            "----------------- End -------------------\n",
            "directory, realimg, fakeimg: /content/dataset/train/ 98657 92799\n",
            ">>> Size of dataset:  191456\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "#training images = 11966\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
            "100% 97.8M/97.8M [00:03<00:00, 25.8MB/s]\n",
            "epoch numbre is = 0\n",
            "/content/gdrive/MyDrive/FusingGlobalandLocal/networks/trainer.py:47: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  scale_base=(scale-scale_min).long()//2 #torch.div(scale-scale_min,2,rounding_mode='floor')\n",
            "/content/gdrive/MyDrive/FusingGlobalandLocal/networks/trainer.py:59: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  loc_max=torch.stack((loc_max_flat//score_width, loc_max_flat%score_width), dim=1)\n",
            "Train loss: 0.29120492935180664 at step: 400\n",
            "Train loss: 0.22872646152973175 at step: 800\n",
            "Train loss: 0.3875381648540497 at step: 1200\n",
            "Train loss: 0.18516963720321655 at step: 1600\n",
            "Train loss: 0.027334656566381454 at step: 2000\n",
            "saving the latest model test_1 (epoch 0, model.total_steps 2000)\n",
            "could not read the, hence skipping it.\n",
            "saving the model at the end of epoch 0, iters 2396\n",
            "directory, realimg, fakeimg: /content/dataset/val/ 12332 11599\n",
            ">>> Size of dataset:  23931\n",
            "number of validation dataset:  1496\n",
            "(Val @ epoch 0) acc: 0.9337261292883707\n",
            "Validation accuracy increased (-inf --> 0.933726).  Saving model ...\n",
            "epoch numbre is = 1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/content/gdrive/MyDrive/FusingGlobalandLocal/networks/trainer.py:47: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  scale_base=(scale-scale_min).long()//2 #torch.div(scale-scale_min,2,rounding_mode='floor')\n",
            "/content/gdrive/MyDrive/FusingGlobalandLocal/networks/trainer.py:59: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  loc_max=torch.stack((loc_max_flat//score_width, loc_max_flat%score_width), dim=1)\n",
            "Train loss: 0.1128191202878952 at step: 2400\n",
            "Train loss: 0.08809100091457367 at step: 2800\n",
            "Train loss: 0.257318913936615 at step: 3200\n",
            "Train loss: 0.15693531930446625 at step: 3600\n",
            "Train loss: 0.17625610530376434 at step: 4000\n",
            "saving the latest model test_1 (epoch 1, model.total_steps 4000)\n",
            "Train loss: 0.2220846563577652 at step: 4400\n",
            "Train loss: 0.04586808383464813 at step: 4800\n",
            "Train loss: 0.09351394325494766 at step: 5200\n",
            "Train loss: 0.022404640913009644 at step: 5600\n",
            "Train loss: 0.19676023721694946 at step: 6000\n",
            "saving the latest model test_1 (epoch 1, model.total_steps 6000)\n",
            "Train loss: 0.037975676357746124 at step: 6400\n",
            "Train loss: 0.08552543818950653 at step: 6800\n",
            "Train loss: 0.35392406582832336 at step: 7200\n",
            "Train loss: 0.1846698820590973 at step: 7600\n",
            "Train loss: 0.08442213386297226 at step: 8000\n",
            "saving the latest model test_1 (epoch 1, model.total_steps 8000)\n",
            "could not read the, hence skipping it.\n",
            "saving the model at the end of epoch 1, iters 8384\n",
            "directory, realimg, fakeimg: /content/dataset/val/ 12332 11599\n",
            ">>> Size of dataset:  23931\n",
            "number of validation dataset:  1496\n",
            "(Val @ epoch 1) acc: 0.9436713885754878\n",
            "Validation accuracy increased (0.933726 --> 0.943671).  Saving model ...\n",
            "epoch numbre is = 2\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/content/gdrive/MyDrive/FusingGlobalandLocal/networks/trainer.py:47: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  scale_base=(scale-scale_min).long()//2 #torch.div(scale-scale_min,2,rounding_mode='floor')\n",
            "/content/gdrive/MyDrive/FusingGlobalandLocal/networks/trainer.py:59: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  loc_max=torch.stack((loc_max_flat//score_width, loc_max_flat%score_width), dim=1)\n",
            "Train loss: 0.07549774646759033 at step: 8400\n",
            "Train loss: 0.0994066596031189 at step: 8800\n",
            "could not read the, hence skipping it.\n",
            "saving the model at the end of epoch 2, iters 9153\n",
            "directory, realimg, fakeimg: /content/dataset/val/ 12332 11599\n",
            ">>> Size of dataset:  23931\n",
            "number of validation dataset:  1496\n",
            "(Val @ epoch 2) acc: 0.9629768918975388\n",
            "Validation accuracy increased (0.943671 --> 0.962977).  Saving model ...\n",
            "epoch numbre is = 3\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/content/gdrive/MyDrive/FusingGlobalandLocal/networks/trainer.py:47: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  scale_base=(scale-scale_min).long()//2 #torch.div(scale-scale_min,2,rounding_mode='floor')\n",
            "/content/gdrive/MyDrive/FusingGlobalandLocal/networks/trainer.py:59: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  loc_max=torch.stack((loc_max_flat//score_width, loc_max_flat%score_width), dim=1)\n",
            "Train loss: 0.029103033244609833 at step: 9200\n",
            "Train loss: 0.054083891212940216 at step: 9600\n",
            "Train loss: 0.1984269618988037 at step: 10000\n",
            "saving the latest model test_1 (epoch 3, model.total_steps 10000)\n",
            "Train loss: 0.01010167971253395 at step: 10400\n",
            "Train loss: 0.06480538845062256 at step: 10800\n",
            "Train loss: 0.04678747057914734 at step: 11200\n",
            "Train loss: 0.12259504199028015 at step: 11600\n",
            "Train loss: 0.05073061212897301 at step: 12000\n",
            "saving the latest model test_1 (epoch 3, model.total_steps 12000)\n",
            "Train loss: 0.03901618346571922 at step: 12400\n",
            "Train loss: 0.028922680765390396 at step: 12800\n",
            "Train loss: 0.20793864130973816 at step: 13200\n",
            "Train loss: 0.03269142657518387 at step: 13600\n",
            "could not read the, hence skipping it.\n",
            "saving the model at the end of epoch 3, iters 13650\n",
            "directory, realimg, fakeimg: /content/dataset/val/ 12332 11599\n",
            ">>> Size of dataset:  23931\n",
            "number of validation dataset:  1496\n",
            "(Val @ epoch 3) acc: 0.9585892775061635\n",
            "EarlyStopping counter: 1 out of 10\n",
            "epoch numbre is = 4\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/content/gdrive/MyDrive/FusingGlobalandLocal/networks/trainer.py:47: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  scale_base=(scale-scale_min).long()//2 #torch.div(scale-scale_min,2,rounding_mode='floor')\n",
            "/content/gdrive/MyDrive/FusingGlobalandLocal/networks/trainer.py:59: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  loc_max=torch.stack((loc_max_flat//score_width, loc_max_flat%score_width), dim=1)\n",
            "Train loss: 0.12151767313480377 at step: 14000\n",
            "saving the latest model test_1 (epoch 4, model.total_steps 14000)\n",
            "Train loss: 0.1644720584154129 at step: 14400\n",
            "Train loss: 0.08243868499994278 at step: 14800\n",
            "Train loss: 0.04732707515358925 at step: 15200\n"
          ]
        }
      ],
      "source": [
        "!export CUDA_VISIBLE_DEVICES=0;\n",
        "!python train.py --name test_2 --blur_prob 0.1 --blur_sig 0.0,3.0 --jpg_prob 0.1 --jpg_method cv2,pil --niter 5 --jpg_qual 30,100 --dataroot /content/dataset --batch_size 16 --lr 0.0001 --gpu_ids 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0B6NIixqy2D"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNPlJV3rQ31s",
        "outputId": "819b863c-eafd-4a15-c976-fdcdc91e2f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing progan-generated images\n",
            "directory, realimg, fakeimg: /content/test/progan 12333 11601\n",
            ">>> Size of dataset:  23934\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/content/gdrive/MyDrive/FusingGlobalandLocal/networks/trainer.py:47: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  scale_base=(scale-scale_min).long()//2 #torch.div(scale-scale_min,2,rounding_mode='floor')\n",
            "/content/gdrive/MyDrive/FusingGlobalandLocal/networks/trainer.py:59: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  loc_max=torch.stack((loc_max_flat//score_width, loc_max_flat%score_width), dim=1)\n",
            "batch number 50/374\n",
            "batch number 100/374\n",
            "batch number 150/374\n",
            "batch number 200/374\n",
            "batch number 250/374\n",
            "batch number 300/374\n",
            "batch number 350/374\n",
            "oa: 0.9622294643603242; auc: 0.9908633459037218; ap:0.9928107248372684\n",
            "Traceback (most recent call last):\n",
            "  File \"eval.py\", line 76, in <module>\n",
            "    with open(csv_name, 'a') as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: \"/content + '/model_epoch_best.csv\"\n"
          ]
        }
      ],
      "source": [
        "!export CUDA_VISIBLE_DEVICES=0;\n",
        "\n",
        "!python eval.py "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PjvtfNbp8E_o",
        "outputId": "3f9ab01a-2425-41df-b437-1078d6dbdcd7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/VIP_cup/83_ValAcc_model.pth'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# shutil.copy(\"/content/FusingGlobalandLocal/checkpoints/test_1/model_epoch_49.pth\",\"/content/gdrive/MyDrive/VIP_cup/83_ValAcc_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nTQV6If0QkML",
        "outputId": "ca2d9ba9-7f40-45dc-83dd-89b5c133feca"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/VIP_cup/best_saved_14thAugust.pth'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# shutil.copy(\"/content/FusingGlobalandLocal/checkpoints/test_1/model_epoch_best.pth\",\"/content/gdrive/MyDrive/VIP_cup/best_saved_14thAugust.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Global_Local_Fusion.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}